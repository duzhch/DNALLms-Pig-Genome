# Base model configuration

# Model parameters
model:
  type: "gpt2"  # Options: gpt2, bert, mamba
  hidden_size: 768
  num_layers: 12
  num_attention_heads: 12
  max_position_embeddings: 1024
  vocab_size: 32000  # Vocabulary size for DNA sequences

# Training parameters
training:
  batch_size: 32
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_steps: 1000
  max_steps: 100000
  save_interval: 1000
  eval_interval: 100

# Data parameters
data:
  train_path: "data/train"
  valid_path: "data/valid"
  test_path: "data/test"
  max_seq_length: 1024

# Distributed training
distributed:
  backend: "nccl"
  world_size: 1
  local_rank: 0

# Logging and checkpointing
logging:
  wandb_project: "dna-llm"
  log_interval: 10
  checkpoint_dir: "checkpoints"